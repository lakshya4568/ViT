{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10959273,"sourceType":"datasetVersion","datasetId":6817955},{"sourceId":10964515,"sourceType":"datasetVersion","datasetId":6821743},{"sourceId":278955,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238950,"modelId":260612}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nimport requests\nimport zipfile\nfrom tqdm import tqdm\nimport random\nimport time\n\n# Set random seeds for reproducibility\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seeds()\n\n# Select device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data preparation functions\ndef download_data(source, destination):\n    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n    dest_path = Path(destination)\n    if not dest_path.exists():\n        dest_path.mkdir(parents=True, exist_ok=True)\n    file_name = source.split('/')[-1]\n    file_path = dest_path / file_name\n    if not file_path.exists():\n        with open(file_path, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"Downloading {file_name}...\")\n            f.write(request.content)\n    else:\n        print(f\"{file_name} already exists\")\n\n    extract_path = dest_path / \"tiny-imagenet-200\"\n    if not extract_path.exists():\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n            print(f\"Extracting {file_name}...\")\n            zip_ref.extractall(dest_path)\n    else:\n        print(f\"{extract_path} already exists\")\n\n    return dest_path / \"tiny-imagenet-200\"\n\nclass CustomImageDataset(Dataset):\n    \"\"\"Custom dataset for TinyImageNet validation set.\"\"\"\n    def __init__(self, img_dir, annotation_file, class_to_idx, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.annotations = self._load_annotations(annotation_file)\n        self.class_to_idx = class_to_idx\n        \n    def _load_annotations(self, annotation_file):\n        annotations = {}\n        with open(annotation_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                img_name = parts[0]\n                class_name = parts[1]\n                annotations[img_name] = class_name\n        return annotations\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        img_name = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        label = self.annotations[img_name]\n        label_idx = self.class_to_idx[label]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label_idx\n\n# Patch Embedding Layer\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convert input images to patch embeddings.\"\"\"\n    def __init__(self, img_size=128, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Use convolution for patch embedding\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n    def forward(self, x):\n        # Input: [batch_size, in_channels, img_size, img_size]\n        # Output: [batch_size, num_patches, embed_dim]\n        \n        # Apply convolution to create patches\n        x = self.proj(x)  # [batch_size, embed_dim, grid_size, grid_size]\n        \n        # Flatten spatial dimensions\n        x = x.flatten(2)  # [batch_size, embed_dim, num_patches]\n        \n        # Transpose to get [batch_size, num_patches, embed_dim]\n        x = x.transpose(1, 2)\n        \n        return x\n\n# Multi-head Self-Attention\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention module with optimizations.\"\"\"\n    def __init__(self, dim, num_heads, dropout=0.0, qkv_bias=True):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n        \n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        # Combined projection for Q, K, V for efficiency\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # x shape: [batch_size, num_patches+1, embed_dim]\n        batch_size, num_tokens, dim = x.shape\n        \n        # Calculate query, key, value vectors\n        qkv = self.qkv(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch_size, num_heads, num_tokens, head_dim]\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Calculate attention scores\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # [batch_size, num_heads, num_tokens, num_tokens]\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(batch_size, num_tokens, dim)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        \n        return x\n\n# MLP Block\nclass MLP(nn.Module):\n    \"\"\"MLP layer with GELU activation.\"\"\"\n    def __init__(self, dim, hidden_dim, dropout=0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        return x\n\n# Transformer Block\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer block with pre-norm architecture.\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0, qkv_bias=True):\n        super().__init__()\n        \n        # Pre-norm architecture with LayerNorm\n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = MultiHeadAttention(dim, num_heads, attn_dropout, qkv_bias=qkv_bias)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n        \n        # Dropout for residual connections\n        self.drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # Self-attention block with residual connection and pre-norm\n        x = x + self.drop(self.attn(self.norm1(x)))\n        \n        # MLP block with residual connection and pre-norm\n        x = x + self.drop(self.mlp(self.norm2(x)))\n        \n        return x\n\n# Vision Transformer\nclass VisionTransformer(nn.Module):\n    \"\"\"Optimized Vision Transformer implementation.\"\"\"\n    def __init__(\n        self,\n        img_size=128,\n        patch_size=16,\n        in_channels=3,\n        num_classes=200,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        dropout=0.0,\n        attn_dropout=0.0,\n        embed_dropout=0.0\n    ):\n        super().__init__()\n        \n        # Patch embedding layer\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n        \n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # Position embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        \n        # Dropout after position embeddings\n        self.pos_drop = nn.Dropout(embed_dropout)\n        \n        # Transformer blocks\n        self.blocks = nn.Sequential(*[\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                qkv_bias=qkv_bias\n            )\n            for _ in range(depth)\n        ])\n        \n        # Final normalization layer\n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        \n        # Classification head\n        self.head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n    \n    def forward_features(self, x):\n        # Get patch embeddings\n        x = self.patch_embed(x)  # [B, num_patches, embed_dim]\n        \n        # Add class token\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # [B, 1, embed_dim]\n        x = torch.cat((cls_token, x), dim=1)  # [B, num_patches+1, embed_dim]\n        \n        # Add position embeddings\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        # Apply transformer blocks\n        x = self.blocks(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Use only the class token for classification\n        return x[:, 0]\n    \n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n# Training and evaluation functions\ndef train_one_epoch(model, dataloader, criterion, optimizer, scheduler=None):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    progress_bar = tqdm(dataloader, desc=\"Training\")\n    \n    for batch_idx, (images, labels) in enumerate(progress_bar):\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update statistics\n        running_loss += loss.item()\n        \n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': running_loss / (batch_idx + 1), \n            'acc': 100. * correct / total\n        })\n        \n    # Update learning rate\n    if scheduler:\n        scheduler.step()\n        \n    return running_loss / len(dataloader), 100. * correct / total\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(dataloader), 100. * correct / total\n\n# Main function to run the training\ndef main():\n    # Data parameters\n    IMG_SIZE = 128\n    BATCH_SIZE = 64\n    NUM_WORKERS = 4\n    \n    # Model parameters\n    PATCH_SIZE = 16\n    EMBED_DIM = 768\n    DEPTH = 12\n    NUM_HEADS = 12\n    MLP_RATIO = 4.0\n    DROPOUT = 0.1\n    ATTN_DROPOUT = 0.0\n    EMBED_DROPOUT = 0.1\n    \n    # Training parameters\n    EPOCHS = 30\n    LEARNING_RATE = 2e-4\n    WEIGHT_DECAY = 0.05\n    \n    # Data augmentation for training\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Validation transforms - only resize and normalize\n    val_transform = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Check if TinyImageNet is already downloaded, otherwise download it\n    if os.path.exists(\"tiny-imagenet-200\"):\n        print(\"Using existing TinyImageNet dataset\")\n        image_path = Path(\"tiny-imagenet-200\")\n    else:\n        print(\"Downloading TinyImageNet dataset\")\n        image_path = download_data(\n            source=\"https://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n            destination=\".\"\n        )\n    \n    train_dir = image_path / \"train\"\n    val_dir = image_path / \"val\"\n    val_img_dir = val_dir / \"images\"\n    val_annotations_file = val_dir / \"val_annotations.txt\"\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n    class_to_idx = train_dataset.class_to_idx\n    \n    val_dataset = CustomImageDataset(\n        img_dir=val_img_dir,\n        annotation_file=val_annotations_file,\n        class_to_idx=class_to_idx,\n        transform=val_transform\n    )\n    \n    # For faster training, use a subset of the data\n    # Comment these lines if you want to use the full dataset\n    subset_size = 90000\n    train_indices = list(range(min(len(train_dataset), subset_size)))\n    val_indices = list(range(min(len(val_dataset), 2000)))\n    \n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(val_dataset, val_indices)\n    \n    print(f\"Train dataset size: {len(train_subset)}\")\n    print(f\"Validation dataset size: {len(val_subset)}\")\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_subset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_subset, \n        batch_size=BATCH_SIZE, \n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n    \n    # Create model\n    print(\"Initializing model...\")\n    model = VisionTransformer(\n        img_size=IMG_SIZE,\n        patch_size=PATCH_SIZE,\n        in_channels=3,\n        num_classes=len(class_to_idx),\n        embed_dim=EMBED_DIM,\n        depth=DEPTH,\n        num_heads=NUM_HEADS,\n        mlp_ratio=MLP_RATIO,\n        dropout=DROPOUT,\n        attn_dropout=ATTN_DROPOUT,\n        embed_dropout=EMBED_DROPOUT\n    )\n    \n    # Move model to device\n    model = model.to(device)\n    print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n    \n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY\n    )\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    \n    # Train model\n    print(\"Starting training...\")\n    best_acc = 0.0\n    \n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n        print(\"-\" * 20)\n        \n        # Train\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, scheduler\n        )\n        \n        # Evaluate\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if train_acc > best_acc:\n            best_acc = train_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n            }, \"best_model.pt\")\n            print(f\"Saved model with train accuracy: {train_acc:.2f}%\")\n    \n    print(f\"Training completed. Best training accuracy: {best_acc:.2f}%\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:10:03.615508Z","iopub.execute_input":"2025-03-08T02:10:03.615793Z","iopub.status.idle":"2025-03-08T04:58:21.242339Z","shell.execute_reply.started":"2025-03-08T02:10:03.615769Z","shell.execute_reply":"2025-03-08T04:58:21.241004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nimport requests\nimport zipfile\nfrom tqdm import tqdm\nimport random\nimport time\n\n# Set random seeds\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seeds()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data preparation functions\ndef download_data(source, destination):\n    dest_path = Path(destination)\n    if not dest_path.exists():\n        dest_path.mkdir(parents=True, exist_ok=True)\n    file_name = source.split('/')[-1]\n    file_path = dest_path / file_name\n    if not file_path.exists():\n        with open(file_path, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"Downloading {file_name}...\")\n            f.write(request.content)\n    else:\n        print(f\"{file_name} already exists\")\n\n    extract_path = dest_path / \"tiny-imagenet-200\"\n    if not extract_path.exists():\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n            print(f\"Extracting {file_name}...\")\n            zip_ref.extractall(dest_path)\n    else:\n        print(f\"{extract_path} already exists\")\n\n    return dest_path / \"tiny-imagenet-200\"\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, img_dir, annotation_file, class_to_idx, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.annotations = self._load_annotations(annotation_file)\n        self.class_to_idx = class_to_idx\n        \n    def _load_annotations(self, annotation_file):\n        annotations = {}\n        with open(annotation_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                img_name = parts[0]\n                class_name = parts[1]\n                annotations[img_name] = class_name\n        return annotations\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        img_name = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        label = self.annotations[img_name]\n        label_idx = self.class_to_idx[label]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label_idx\n\n# OPTIMIZATION 1: Improved PatchEmbedding with layer normalization\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=128, patch_size=16, in_channels=3, embed_dim=512):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Use convolution for patch embedding\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        \n        # ADDED: Normalization after projection for stable training\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        x = self.norm(x)  # Apply normalization\n        return x\n\n# OPTIMIZATION 2: Modified MultiHeadAttention with more stable softmax scaling\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, num_heads, dropout=0.1, qkv_bias=True):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n        \n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        # MODIFIED: Adjusted scaling factor for better gradient flow\n        self.scale = (self.head_dim ** -0.5) * 0.8\n        \n        # Combined projection for Q, K, V\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        batch_size, num_tokens, dim = x.shape\n        \n        qkv = self.qkv(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Calculate attention scores\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # ADDED: Apply attention mask to avoid gradient issues with padding\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(batch_size, num_tokens, dim)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        \n        return x\n\n# OPTIMIZATION 3: Enhanced MLP with mixture of GELU and ReLU activations\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        # MODIFIED: Using GELU for better feature learning\n        self.act = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        return x\n\n# OPTIMIZATION 4: Improved TransformerBlock with StochDepth (drop path)\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1, attn_dropout=0.1, \n                 drop_path=0.0, qkv_bias=True):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = MultiHeadAttention(dim, num_heads, attn_dropout, qkv_bias=qkv_bias)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n        \n        # ADDED: Stochastic depth (drop path) for enhanced regularization\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        \n    def forward(self, x):\n        # Using drop path instead of dropout for residual connections\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n# ADDED: DropPath (Stochastic Depth) implementation\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=0.):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if self.drop_prob == 0. or not self.training:\n            return x\n            \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\n# OPTIMIZATION 5: Enhanced Vision Transformer with deeper architecture\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=128,\n        patch_size=16,\n        in_channels=3,\n        num_classes=200,\n        embed_dim=512,  # REDUCED from 768 to limit model size\n        depth=8,        # REDUCED from 12 for efficiency\n        num_heads=8,    # REDUCED from 12\n        mlp_ratio=3.0,  # REDUCED from 4.0\n        qkv_bias=True,\n        dropout=0.1,\n        attn_dropout=0.1,\n        embed_dropout=0.1,\n        drop_path_rate=0.1  # ADDED: drop path rate\n    ):\n        super().__init__()\n        \n        # Patch embedding layer\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n        \n        # Class token and position embedding\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # ADDED: Use learnable absolute positional embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(embed_dropout)\n        \n        # MODIFIED: Stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        \n        # Transformer blocks with progressively increasing drop path rate\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                drop_path=dpr[i],\n                qkv_bias=qkv_bias\n            )\n            for i in range(depth)\n        ])\n        \n        # Final normalization and classification head\n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        \n        # MODIFIED: Two-stage head for better classification\n        self.pre_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.BatchNorm1d(embed_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.2)\n        )\n        self.head = nn.Linear(embed_dim // 2, num_classes)\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n        # OPTIMIZATION 6: Better initialization\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n    \n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        \n        # Add class token for classification\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        \n        # Add positional embedding\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        # Apply transformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        # Apply final layer normalization\n        x = self.norm(x)\n        \n        # Use class token for classification\n        return x[:, 0]\n    \n    def forward(self, x):\n        x = self.forward_features(x)\n        \n        # Apply two-stage head\n        x = self.pre_head(x)\n        x = self.head(x)\n        return x\n\n# OPTIMIZATION 7: Enhanced training function with gradient clipping\ndef train_one_epoch(model, dataloader, criterion, optimizer, scheduler=None, clip_grad_norm=1.0):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    progress_bar = tqdm(dataloader, desc=\"Training\")\n    \n    for batch_idx, (images, labels) in enumerate(progress_bar):\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        \n        # Apply gradient clipping to prevent exploding gradients\n        if clip_grad_norm > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n            \n        optimizer.step()\n        \n        # Update statistics\n        running_loss += loss.item()\n        \n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': running_loss / (batch_idx + 1), \n            'acc': 100. * correct / total\n        })\n        \n    # Update learning rate\n    if scheduler:\n        scheduler.step()\n        \n    return running_loss / len(dataloader), 100. * correct / total\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(dataloader), 100. * correct / total\n\n# Main function to run the training\ndef main():\n    # MODIFIED: Data parameters\n    IMG_SIZE = 128\n    BATCH_SIZE = 128  # INCREASED for faster training\n    NUM_WORKERS = 4\n    \n    # MODIFIED: Model parameters for better performance/efficiency balance\n    PATCH_SIZE = 16\n    EMBED_DIM = 512  # Reduced from 768\n    DEPTH = 8        # Reduced from 12\n    NUM_HEADS = 8    # Reduced from 12\n    MLP_RATIO = 3.0  # Reduced from 4.0\n    DROPOUT = 0.1\n    ATTN_DROPOUT = 0.1\n    EMBED_DROPOUT = 0.1\n    \n    # MODIFIED: Training parameters\n    EPOCHS = 30\n    LEARNING_RATE = 5e-4  # Increased for faster convergence\n    WEIGHT_DECAY = 0.1    # Increased for better regularization\n    \n    # OPTIMIZATION 8: Enhanced data augmentation\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),  # More aggressive crop\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),  # Add rotation for robustness\n        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomErasing(p=0.2)  # Add random erasing for robustness\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Dataset preparation\n    if os.path.exists(\"tiny-imagenet-200\"):\n        print(\"Using existing TinyImageNet dataset\")\n        image_path = Path(\"tiny-imagenet-200\")\n    else:\n        print(\"Downloading TinyImageNet dataset\")\n        image_path = download_data(\n            source=\"https://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n            destination=\".\"\n        )\n    \n    train_dir = image_path / \"train\"\n    val_dir = image_path / \"val\"\n    val_img_dir = val_dir / \"images\"\n    val_annotations_file = val_dir / \"val_annotations.txt\"\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n    class_to_idx = train_dataset.class_to_idx\n    \n    val_dataset = CustomImageDataset(\n        img_dir=val_img_dir,\n        annotation_file=val_annotations_file,\n        class_to_idx=class_to_idx,\n        transform=val_transform\n    )\n    \n    # MODIFIED: Using more data for better training\n    subset_size = 90000  # INCREASED from 10000 to use more training data\n    train_indices = list(range(min(len(train_dataset), subset_size)))\n    val_indices = list(range(min(len(val_dataset), 5000)))  # INCREASED validation set\n    \n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(val_dataset, val_indices)\n    \n    print(f\"Train dataset size: {len(train_subset)}\")\n    print(f\"Validation dataset size: {len(val_subset)}\")\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_subset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_subset, \n        batch_size=BATCH_SIZE, \n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n    \n    # Create model\n    print(\"Initializing model...\")\n    model = VisionTransformer(\n        img_size=IMG_SIZE,\n        patch_size=PATCH_SIZE,\n        in_channels=3,\n        num_classes=len(class_to_idx),\n        embed_dim=EMBED_DIM,\n        depth=DEPTH,\n        num_heads=NUM_HEADS,\n        mlp_ratio=MLP_RATIO,\n        dropout=DROPOUT,\n        attn_dropout=ATTN_DROPOUT,\n        embed_dropout=EMBED_DROPOUT\n    )\n    \n    # Move model to device\n    model = model.to(device)\n    print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n    \n    # OPTIMIZATION 9: Label smoothing for better generalization\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    # OPTIMIZATION 10: Parameter groups with different weight decay\n    no_decay = ['bias', 'LayerNorm.weight', 'BatchNorm1d.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() \n                       if not any(nd in n for nd in no_decay)],\n            'weight_decay': WEIGHT_DECAY\n        },\n        {\n            'params': [p for n, p in model.named_parameters() \n                       if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0\n        }\n    ]\n    \n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n    \n    # OPTIMIZATION 11: OneCycleLR scheduler for faster convergence\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=LEARNING_RATE,\n        total_steps=EPOCHS * len(train_loader),\n        pct_start=0.2,  # Warm up for 20% of training\n        div_factor=25,\n        final_div_factor=1000\n    )\n    \n    # Train model with early stopping\n    print(\"Starting training...\")\n    best_acc = 0.0\n    early_stop_patience = 5\n    early_stop_counter = 0\n    \n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n        print(\"-\" * 20)\n        \n        # Train\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, scheduler, clip_grad_norm=1.0\n        )\n        \n        # Evaluate\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model based on validation accuracy\n        if val_acc > best_acc:\n            best_acc = val_acc\n            early_stop_counter = 0\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n            }, \"best_model.pt\")\n            print(f\"Saved model with val accuracy: {val_acc:.2f}%\")\n        else:\n            early_stop_counter += 1\n            \n        # Early stopping\n        if early_stop_counter >= early_stop_patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    print(f\"Training completed. Best validation accuracy: {best_acc:.2f}%\")\n    \n    # Load best model for final evaluation\n    checkpoint = torch.load(\"/kaggle/input/best-model/best_model_v2.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Final evaluation\n    _, final_val_acc = evaluate(model, val_loader, criterion)\n    print(f\"Final model validation accuracy: {final_val_acc:.2f}%\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T05:06:39.419675Z","iopub.execute_input":"2025-03-08T05:06:39.420099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nimport requests\nimport zipfile\nfrom tqdm import tqdm\nimport random\nimport time\n\n# Import PyTorch/XLA libraries for TPU support\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\n# Model classes and helper functions (all unchanged from your code)\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=0.):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if self.drop_prob == 0. or not self.training:\n            return x\n            \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=128, patch_size=16, in_channels=3, embed_dim=512):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        x = self.norm(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, num_heads, dropout=0.1, qkv_bias=True):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n        \n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = (self.head_dim ** -0.5) * 0.8\n        \n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        batch_size, num_tokens, dim = x.shape\n        \n        qkv = self.qkv(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(batch_size, num_tokens, dim)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        \n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1, attn_dropout=0.1, \n                 drop_path=0.0, qkv_bias=True):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = MultiHeadAttention(dim, num_heads, attn_dropout, qkv_bias=qkv_bias)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n        \n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        \n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=128,\n        patch_size=16,\n        in_channels=3,\n        num_classes=200,\n        embed_dim=512,\n        depth=8,\n        num_heads=8,\n        mlp_ratio=3.0,\n        qkv_bias=True,\n        dropout=0.1,\n        attn_dropout=0.1,\n        embed_dropout=0.1,\n        drop_path_rate=0.1\n    ):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(embed_dropout)\n        \n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                drop_path=dpr[i],\n                qkv_bias=qkv_bias\n            )\n            for i in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        \n        self.pre_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.BatchNorm1d(embed_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.2)\n        )\n        self.head = nn.Linear(embed_dim // 2, num_classes)\n        \n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize position embedding and class token\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n        # Initialize all linear layers\n        self.apply(self._init_layer_weights)\n    \n    def _init_layer_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n    \n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        \n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        \n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        \n        return x[:, 0]\n    \n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.pre_head(x)\n        x = self.head(x)\n        return x\n\n# Data preparation functions\ndef download_data(source, destination):\n    dest_path = Path(destination)\n    if not dest_path.exists():\n        dest_path.mkdir(parents=True, exist_ok=True)\n    file_name = source.split('/')[-1]\n    file_path = dest_path / file_name\n    if not file_path.exists():\n        with open(file_path, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"Downloading {file_name}...\")\n            f.write(request.content)\n    else:\n        print(f\"{file_name} already exists\")\n\n    extract_path = dest_path / \"tiny-imagenet-200\"\n    if not extract_path.exists():\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n            print(f\"Extracting {file_name}...\")\n            zip_ref.extractall(dest_path)\n    else:\n        print(f\"{extract_path} already exists\")\n\n    return dest_path / \"tiny-imagenet-200\"\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, img_dir, annotation_file, class_to_idx, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.annotations = self._load_annotations(annotation_file)\n        self.class_to_idx = class_to_idx\n        \n    def _load_annotations(self, annotation_file):\n        annotations = {}\n        with open(annotation_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                img_name = parts[0]\n                class_name = parts[1]\n                annotations[img_name] = class_name\n        return annotations\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        img_name = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        label = self.annotations[img_name]\n        label_idx = self.class_to_idx[label]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label_idx\n\n# TPU training function (moved entirely inside _mp_fn)\ndef _mp_fn(index):\n    # IMPORTANT: Only initialize the device after xmp.spawn is called\n    device = xm.xla_device()\n    \n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    \n    # Print TPU device info\n    xm.master_print(f\"Using device: {device}\")\n    xm.master_print(f\"TPU process {index} initialized\")\n    \n    # Parameters\n    IMG_SIZE = 128\n    PATCH_SIZE = 16\n    BATCH_SIZE = 64  # Adjusted for TPU compatibility\n    NUM_WORKERS = 2  # Reduced to avoid warnings\n    \n    EMBED_DIM = 512\n    DEPTH = 8\n    NUM_HEADS = 8\n    MLP_RATIO = 3.0\n    DROPOUT = 0.1\n    ATTN_DROPOUT = 0.1\n    EMBED_DROPOUT = 0.1\n    \n    # Continue training from checkpoint\n    START_EPOCH = 28  # Already completed 29 epochs\n    EPOCHS = 100       # Train for more epochs\n    LEARNING_RATE = 1e-4  # Reduced learning rate for fine-tuning\n    WEIGHT_DECAY = 0.1\n    \n    # Only execute dataset preparation on the master process\n    if xm.is_master_ordinal():\n        # Data augmentation\n        train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.2)\n        ])\n        \n        val_transform = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load dataset\n        if os.path.exists(\"tiny-imagenet-200\"):\n            xm.master_print(\"Using existing TinyImageNet dataset\")\n            image_path = Path(\"tiny-imagenet-200\")\n        else:\n            xm.master_print(\"Downloading TinyImageNet dataset\")\n            image_path = download_data(\n                source=\"https://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n                destination=\".\"\n            )\n        \n        train_dir = image_path / \"train\"\n        val_dir = image_path / \"val\"\n        val_img_dir = val_dir / \"images\"\n        val_annotations_file = val_dir / \"val_annotations.txt\"\n        \n        # Create datasets\n        xm.master_print(\"Creating datasets...\")\n        train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n        class_to_idx = train_dataset.class_to_idx\n        \n        val_dataset = CustomImageDataset(\n            img_dir=val_img_dir,\n            annotation_file=val_annotations_file,\n            class_to_idx=class_to_idx,\n            transform=val_transform\n        )\n        \n        # Use full dataset\n        xm.master_print(f\"Train dataset size: {len(train_dataset)}\")\n        xm.master_print(f\"Validation dataset size: {len(val_dataset)}\")\n\n    # Synchronize all processes to ensure master has fully prepared the data\n    xm.rendezvous('dataset_prepared')\n    \n    # Make datasets available to all processes\n    if xm.is_master_ordinal():\n        world_size = xm.xrt_world_size()\n        xm.master_print(f\"TPU world size: {world_size}\")\n    \n    # Create datasets and data loaders (must be done in all processes)\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n    class_to_idx = train_dataset.class_to_idx\n    \n    val_dataset = CustomImageDataset(\n        img_dir=val_img_dir,\n        annotation_file=val_annotations_file,\n        class_to_idx=class_to_idx,\n        transform=val_transform\n    )\n\n    # Create TPU-optimized samplers\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        num_workers=NUM_WORKERS,\n        drop_last=True\n    )\n    \n    # For validation, we don't need to distribute it as much\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        val_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        num_workers=NUM_WORKERS\n    )\n    \n    # Create parallel loaders (essential for TPU training)\n    train_device_loader = pl.MpDeviceLoader(train_loader, device)\n    val_device_loader = pl.MpDeviceLoader(val_loader, device)\n    \n    # Initialize model\n    xm.master_print(\"Initializing model...\")\n    model = VisionTransformer(\n        img_size=IMG_SIZE,\n        patch_size=PATCH_SIZE,\n        in_channels=3,\n        num_classes=len(class_to_idx),\n        embed_dim=EMBED_DIM,\n        depth=DEPTH,\n        num_heads=NUM_HEADS,\n        mlp_ratio=MLP_RATIO,\n        dropout=DROPOUT,\n        attn_dropout=ATTN_DROPOUT,\n        embed_dropout=EMBED_DROPOUT\n    )\n    \n    # Move model to TPU\n    model = model.to(device)\n    \n    if xm.is_master_ordinal():\n        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        xm.master_print(f\"Model has {param_count:,} parameters\")\n    \n    # Load checkpoint\n    if os.path.exists('/kaggle/input/best-model/best_model_v2.pt'):\n        xm.master_print(\"Loading checkpoint with 18% accuracy...\")\n        # Use local loading first to avoid TPU issues\n        checkpoint = torch.load('/kaggle/input/best-model/best_model_v2.pt', map_location='cpu')\n        model.load_state_dict(checkpoint['model_state_dict'])\n        xm.master_print(f\"Loaded checkpoint from epoch {checkpoint['epoch']} with {checkpoint['val_acc']:.2f}% accuracy\")\n        best_acc = checkpoint['val_acc']\n    else:\n        xm.master_print(\"No checkpoint found, starting from scratch\")\n        best_acc = 0.0\n    \n    # Loss function with label smoothing\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    # Parameter groups for weight decay\n    no_decay = ['bias', 'LayerNorm.weight', 'BatchNorm1d.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() \n                      if not any(nd in n for nd in no_decay)],\n            'weight_decay': WEIGHT_DECAY\n        },\n        {\n            'params': [p for n, p in model.named_parameters() \n                      if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0\n        }\n    ]\n    \n    # Create optimizer and scheduler\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(EPOCHS - START_EPOCH))\n    \n    # Training and evaluation functions for TPU\n    def train_one_epoch(model, dataloader, criterion, optimizer, scheduler):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (images, labels) in enumerate(dataloader):\n            # Forward pass and loss calculation\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # TPU-specific: we need to call optimizer_step through XLA\n            xm.optimizer_step(optimizer)\n            \n            # Update metrics\n            running_loss += loss.detach().item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            # Log progress (only on master process)\n            if batch_idx % 2 == 0 and xm.is_master_ordinal():\n                xm.master_print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n        \n        # Sync metrics across all TPU cores\n        running_loss = xm.mesh_reduce('train_loss', running_loss, lambda x: sum(x) / len(x))\n        correct = xm.mesh_reduce('train_correct', correct, sum)\n        total = xm.mesh_reduce('train_total', total, sum)\n        \n        # Update scheduler after epoch (if needed)\n        if scheduler:\n            scheduler.step()\n            \n        return running_loss / len(dataloader), 100. * correct / total\n    \n    def evaluate(model, dataloader, criterion):\n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in dataloader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        # Sync metrics across all TPU cores\n        running_loss = xm.mesh_reduce('val_loss', running_loss, lambda x: sum(x) / len(x))\n        correct = xm.mesh_reduce('val_correct', correct, sum)\n        total = xm.mesh_reduce('val_total', total, sum)\n        \n        return running_loss / len(dataloader), 100. * correct / total\n    \n    # Resume training\n    patience = 7  # Increased patience for fine-tuning\n    early_stop_counter = 0\n    \n    for epoch in range(START_EPOCH, EPOCHS):\n        if xm.is_master_ordinal():\n            xm.master_print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n            xm.master_print(\"-\" * 20)\n        \n        # Train\n        train_loss, train_acc = train_one_epoch(\n            model, train_device_loader, criterion, optimizer, scheduler\n        )\n        \n        # Evaluate\n        val_loss, val_acc = evaluate(model, val_device_loader, criterion)\n        \n        if xm.is_master_ordinal():\n            xm.master_print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n            xm.master_print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_acc and xm.is_master_ordinal():\n            best_acc = val_acc\n            early_stop_counter = 0\n            \n            # Save checkpoint (use xm.save to handle TPU serialization)\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n            }\n            xm.save(checkpoint, \"best_tpu_model.pt\")\n            xm.master_print(f\"Saved best model with val accuracy: {val_acc:.2f}%\")\n        elif val_acc <= best_acc:\n            early_stop_counter += 1\n            if xm.is_master_ordinal():\n                xm.master_print(f\"Val accuracy didn't improve. Counter: {early_stop_counter}/{patience}\")\n            \n        # Early stopping\n        if early_stop_counter >= patience:\n            if xm.is_master_ordinal():\n                xm.master_print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n        \n        # Ensure all TPU processes are synchronized before continuing\n        xm.rendezvous(f'epoch_{epoch}_complete')\n    \n    if xm.is_master_ordinal():\n        xm.master_print(f\"Training completed. Best validation accuracy: {best_acc:.2f}%\")\n\n# Entry point\ndef main():\n    print(\"Starting TPU training...\")\n    # Auto-detect the number of TPU cores available\n    try:\n        num_cores = xm.xrt_world_size()\n        print(f\"Detected {num_cores} TPU cores\")\n    except:\n        # If can't detect, default to 1 (single TPU core)\n        num_cores = 1\n        print(\"Could not detect TPU configuration, using single core\")\n    \n    # Spawn TPU processes\n    xmp.spawn(_mp_fn, nprocs=num_cores)\n    print(\"TPU training completed\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:57:44.491453Z","iopub.execute_input":"2025-03-08T09:57:44.491874Z","iopub.status.idle":"2025-03-08T09:57:45.300433Z","shell.execute_reply.started":"2025-03-08T09:57:44.491837Z","shell.execute_reply":"2025-03-08T09:57:45.299158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nimport requests\nimport zipfile\nfrom tqdm import tqdm\nimport random\nimport time\n\n# Import PyTorch/XLA libraries for TPU support\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\n# Model classes and helper functions (all unchanged from your code)\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=0.):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        \n    def forward(self, x):\n        if self.drop_prob == 0. or not self.training:\n            return x\n            \n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=128, patch_size=16, in_channels=3, embed_dim=512):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(\n            in_channels, embed_dim, \n            kernel_size=patch_size, stride=patch_size\n        )\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        x = self.norm(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, num_heads, dropout=0.1, qkv_bias=True):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n        \n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = (self.head_dim ** -0.5) * 0.8\n        \n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        batch_size, num_tokens, dim = x.shape\n        \n        qkv = self.qkv(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(batch_size, num_tokens, dim)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        \n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1, attn_dropout=0.1, \n                 drop_path=0.0, qkv_bias=True):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = MultiHeadAttention(dim, num_heads, attn_dropout, qkv_bias=qkv_bias)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout)\n        \n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        \n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=128,\n        patch_size=16,\n        in_channels=3,\n        num_classes=200,\n        embed_dim=512,\n        depth=8,\n        num_heads=8,\n        mlp_ratio=3.0,\n        qkv_bias=True,\n        dropout=0.1,\n        attn_dropout=0.1,\n        embed_dropout=0.1,\n        drop_path_rate=0.1\n    ):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(embed_dropout)\n        \n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                dropout=dropout,\n                attn_dropout=attn_dropout,\n                drop_path=dpr[i],\n                qkv_bias=qkv_bias\n            )\n            for i in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        \n        self.pre_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.BatchNorm1d(embed_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.2)\n        )\n        self.head = nn.Linear(embed_dim // 2, num_classes)\n        \n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize position embedding and class token\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n        # Initialize all linear layers\n        self.apply(self._init_layer_weights)\n    \n    def _init_layer_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n    \n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        \n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        \n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        \n        return x[:, 0]\n    \n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.pre_head(x)\n        x = self.head(x)\n        return x\n\n# Data preparation functions\ndef download_data(source, destination):\n    dest_path = Path(destination)\n    if not dest_path.exists():\n        dest_path.mkdir(parents=True, exist_ok=True)\n    file_name = source.split('/')[-1]\n    file_path = dest_path / file_name\n    if not file_path.exists():\n        with open(file_path, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"Downloading {file_name}...\")\n            f.write(request.content)\n    else:\n        print(f\"{file_name} already exists\")\n\n    extract_path = dest_path / \"tiny-imagenet-200\"\n    if not extract_path.exists():\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n            print(f\"Extracting {file_name}...\")\n            zip_ref.extractall(dest_path)\n    else:\n        print(f\"{extract_path} already exists\")\n\n    return dest_path / \"tiny-imagenet-200\"\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, img_dir, annotation_file, class_to_idx, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.annotations = self._load_annotations(annotation_file)\n        self.class_to_idx = class_to_idx\n        \n    def _load_annotations(self, annotation_file):\n        annotations = {}\n        with open(annotation_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                img_name = parts[0]\n                class_name = parts[1]\n                annotations[img_name] = class_name\n        return annotations\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        img_name = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        label = self.annotations[img_name]\n        label_idx = self.class_to_idx[label]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label_idx\n\n# TPU training function\ndef _mp_fn(index):\n    # Initialize the device\n    device = xm.xla_device()\n    \n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    \n    # Print TPU device info (only on master process)\n    if xm.is_master_ordinal():\n        xm.master_print(f\"Using device: {device}\")\n        xm.master_print(f\"TPU process {index} initialized\")\n    \n    # Parameters\n    IMG_SIZE = 128\n    PATCH_SIZE = 16\n    BATCH_SIZE = 64\n    NUM_WORKERS = 2\n    \n    EMBED_DIM = 512\n    DEPTH = 8\n    NUM_HEADS = 8\n    MLP_RATIO = 3.0\n    DROPOUT = 0.1\n    ATTN_DROPOUT = 0.1\n    EMBED_DROPOUT = 0.1\n    \n    # Training parameters\n    MAX_EPOCHS = 100\n    LEARNING_RATE = 8e-5  # Further reduced for fine-tuning\n    WEIGHT_DECAY = 0.1\n    \n    # Only execute dataset preparation on the master process\n    if xm.is_master_ordinal():\n        # Data augmentation\n        train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.2)\n        ])\n        \n        val_transform = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load dataset\n        if os.path.exists(\"tiny-imagenet-200\"):\n            xm.master_print(\"Using existing TinyImageNet dataset\")\n            image_path = Path(\"tiny-imagenet-200\")\n        else:\n            xm.master_print(\"Downloading TinyImageNet dataset\")\n            image_path = download_data(\n                source=\"https://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n                destination=\".\"\n            )\n        \n        train_dir = image_path / \"train\"\n        val_dir = image_path / \"val\"\n        val_img_dir = val_dir / \"images\"\n        val_annotations_file = val_dir / \"val_annotations.txt\"\n        \n        # Create datasets\n        xm.master_print(\"Creating datasets...\")\n\n    # Synchronize all processes\n    xm.rendezvous('dataset_prepared')\n    \n    # Create datasets and data loaders (must be done in all processes)\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n    class_to_idx = train_dataset.class_to_idx\n    \n    val_dataset = CustomImageDataset(\n        img_dir=val_img_dir,\n        annotation_file=val_annotations_file,\n        class_to_idx=class_to_idx,\n        transform=val_transform\n    )\n    \n    # Create TPU-optimized samplers\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        num_workers=NUM_WORKERS,\n        drop_last=True\n    )\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        val_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        num_workers=NUM_WORKERS\n    )\n    \n    # Create parallel loaders\n    train_device_loader = pl.MpDeviceLoader(train_loader, device)\n    val_device_loader = pl.MpDeviceLoader(val_loader, device)\n    \n    # Initialize model\n    if xm.is_master_ordinal():\n        xm.master_print(\"Initializing model...\")\n    \n    model = VisionTransformer(\n        img_size=IMG_SIZE,\n        patch_size=PATCH_SIZE,\n        in_channels=3,\n        num_classes=len(class_to_idx),\n        embed_dim=EMBED_DIM,\n        depth=DEPTH,\n        num_heads=NUM_HEADS,\n        mlp_ratio=MLP_RATIO,\n        dropout=DROPOUT,\n        attn_dropout=ATTN_DROPOUT,\n        embed_dropout=EMBED_DROPOUT\n    )\n    \n    # Move model to TPU\n    model = model.to(device)\n    \n    if xm.is_master_ordinal():\n        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        xm.master_print(f\"Model has {param_count:,} parameters\")\n    \n    # Load checkpoint - Use the best_tpu_model.pt file\n    if os.path.exists('/kaggle/input/tpu-model/best_tpu_model_V2.pt'):\n        if xm.is_master_ordinal():\n            xm.master_print(\"Loading checkpoint from best_tpu_model.pt...\")\n            \n        # Load on CPU first then transfer to TPU\n        checkpoint = torch.load('/kaggle/input/tpu-model/best_tpu_model_V2.pt', map_location='cpu')\n        model.load_state_dict(checkpoint['model_state_dict'])\n        \n        # Get the starting epoch and best accuracy\n        start_epoch = checkpoint['epoch'] + 1\n        best_acc = checkpoint['val_acc']\n        \n        if xm.is_master_ordinal():\n            xm.master_print(f\"Resuming from epoch {start_epoch} with accuracy {best_acc:.2f}%\")\n    else:\n        if xm.is_master_ordinal():\n            xm.master_print(\"No checkpoint found, starting from scratch\")\n        start_epoch = 0\n        best_acc = 0.0\n    \n    # Loss function with label smoothing\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    # Parameter groups for weight decay\n    no_decay = ['bias', 'LayerNorm.weight', 'BatchNorm1d.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() \n                      if not any(nd in n for nd in no_decay)],\n            'weight_decay': WEIGHT_DECAY\n        },\n        {\n            'params': [p for n, p in model.named_parameters() \n                      if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0\n        }\n    ]\n    \n    # Create optimizer and scheduler\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n    \n    # If resuming, load optimizer state if available\n    if os.path.exists('best_tpu_model.pt') and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if xm.is_master_ordinal():\n            xm.master_print(\"Loaded optimizer state from checkpoint\")\n    \n    # Create learning rate scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, \n        T_max=(MAX_EPOCHS - start_epoch)\n    )\n    \n    # Training function - modified to not show batch progress\n    def train_one_epoch(model, dataloader, criterion, optimizer, scheduler):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Use tqdm only on master process and only for visualization\n        if xm.is_master_ordinal():\n            pbar = tqdm(total=len(dataloader), desc=\"Training\", leave=False)\n        \n        for images, labels in dataloader:\n            # Forward pass and loss calculation\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # TPU-specific optimizer step\n            xm.optimizer_step(optimizer)\n            \n            # Update metrics\n            running_loss += loss.detach().item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            # Update progress bar on master process\n            if xm.is_master_ordinal():\n                pbar.update(1)\n        \n        if xm.is_master_ordinal():\n            pbar.close()\n        \n        # Sync metrics across all TPU cores\n        running_loss = xm.mesh_reduce('train_loss', running_loss, lambda x: sum(x) / len(x))\n        correct = xm.mesh_reduce('train_correct', correct, sum)\n        total = xm.mesh_reduce('train_total', total, sum)\n        \n        # Update scheduler after epoch\n        if scheduler:\n            scheduler.step()\n            \n        return running_loss / len(dataloader), 100. * correct / total\n    \n    def evaluate(model, dataloader, criterion):\n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Use tqdm only on master process and only for visualization\n        if xm.is_master_ordinal():\n            pbar = tqdm(total=len(dataloader), desc=\"Validating\", leave=False)\n        \n        with torch.no_grad():\n            for images, labels in dataloader:\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                # Update progress bar on master process\n                if xm.is_master_ordinal():\n                    pbar.update(1)\n        \n        if xm.is_master_ordinal():\n            pbar.close()\n        \n        # Sync metrics across all TPU cores\n        running_loss = xm.mesh_reduce('val_loss', running_loss, lambda x: sum(x) / len(x))\n        correct = xm.mesh_reduce('val_correct', correct, sum)\n        total = xm.mesh_reduce('val_total', total, sum)\n        \n        return running_loss / len(dataloader), 100. * correct / total\n    \n    # Resume training\n    patience = 10  # Increased patience for fine-tuning\n    early_stop_counter = 0\n    \n    # Print training summary header\n    if xm.is_master_ordinal():\n        xm.master_print(\"\\n\" + \"=\"*50)\n        xm.master_print(f\"Resuming training from epoch {start_epoch+1} to {MAX_EPOCHS}\")\n        xm.master_print(f\"Best accuracy so far: {best_acc:.2f}%\")\n        xm.master_print(\"=\"*50 + \"\\n\")\n    \n    for epoch in range(start_epoch, MAX_EPOCHS):\n        epoch_start_time = time.time()\n        \n        if xm.is_master_ordinal():\n            xm.master_print(f\"Epoch {epoch+1}/{MAX_EPOCHS}\")\n        \n        # Set epoch for distributed sampler\n        train_sampler.set_epoch(epoch)\n        \n        # Train\n        train_loss, train_acc = train_one_epoch(\n            model, train_device_loader, criterion, optimizer, scheduler\n        )\n        \n        # Evaluate\n        val_loss, val_acc = evaluate(model, val_device_loader, criterion)\n        \n        # Calculate epoch time\n        epoch_time = time.time() - epoch_start_time\n        \n        if xm.is_master_ordinal():\n            current_lr = optimizer.param_groups[0]['lr']\n            xm.master_print(f\"Epoch {epoch+1}/{MAX_EPOCHS} completed in {epoch_time:.2f}s\")\n            xm.master_print(f\"LR: {current_lr:.6f} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n            xm.master_print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_acc and xm.is_master_ordinal():\n            best_acc = val_acc\n            early_stop_counter = 0\n            \n            # Save checkpoint\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n            }\n            xm.save(checkpoint, \"best_tpu_model.pt\")\n            \n            if xm.is_master_ordinal():\n                xm.master_print(f\" New best model saved with val accuracy: {val_acc:.2f}%\")\n        elif val_acc <= best_acc:\n            early_stop_counter += 1\n            if xm.is_master_ordinal():\n                xm.master_print(f\"! No improvement. Early stopping counter: {early_stop_counter}/{patience}\")\n            \n        # Early stopping\n        if early_stop_counter >= patience:\n            if xm.is_master_ordinal():\n                xm.master_print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n            break\n            \n        # Add a separator between epochs\n        if xm.is_master_ordinal():\n            xm.master_print(\"-\"*50)\n        \n        # Ensure all TPU processes are synchronized before continuing\n        xm.rendezvous(f'epoch_{epoch}_complete')\n    \n    if xm.is_master_ordinal():\n        xm.master_print(f\"\\nTraining completed. Best validation accuracy: {best_acc:.2f}%\")\n\n# Entry point\ndef main():\n    print(\"Starting TPU training...\")\n    # Auto-detect the number of TPU cores available\n    try:\n        num_cores = xm.xrt_world_size()\n        print(f\"Detected {num_cores} TPU cores\")\n    except:\n        # If can't detect, default to 1 (single TPU core)\n        num_cores = 1\n        print(\"Could not detect TPU configuration, using single core\")\n    \n    # Spawn TPU processes\n    xmp.spawn(_mp_fn, nprocs=num_cores)\n    print(\"TPU training completed\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T19:53:11.995821Z","iopub.execute_input":"2025-03-08T19:53:11.996126Z","iopub.status.idle":"2025-03-09T02:47:07.646746Z","shell.execute_reply.started":"2025-03-08T19:53:11.996100Z","shell.execute_reply":"2025-03-09T02:47:07.645149Z"}},"outputs":[],"execution_count":null}]}